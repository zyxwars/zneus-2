{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fPlWiTO26ie"
      },
      "source": [
        "# Animals 10\n",
        "Made by Samuel TvrdoÅˆ and Michal Weis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4qGgxJ66El1"
      },
      "source": [
        "## Import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5w7BwXPUQ4a"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision.transforms import v2\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import shutil\n",
        "import wandb\n",
        "from wandb.integration.keras import WandbCallback\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "import tensorflow as tf\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gt3ly7nET1tf"
      },
      "outputs": [],
      "source": [
        "def make_data_frame(path):\n",
        "  data = []\n",
        "  for animal_class in path.iterdir():\n",
        "    if animal_class.is_dir():\n",
        "      for img in animal_class.glob(\"*\"):\n",
        "        data.append({\"label\": animal_class.name, \"path\": str(img)})\n",
        "\n",
        "  return pd.DataFrame(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tF9RO1C_XX5D",
        "outputId": "de04735d-7e08-4b47-ca6f-74b9e49e0014"
      },
      "outputs": [],
      "source": [
        "dataset_url = \"https://www.kaggle.com/api/v1/datasets/download/alessiocorrado99/animals10\"\n",
        "archive = tf.keras.utils.get_file(origin=dataset_url, extract=True)\n",
        "data_dir = Path(archive).with_suffix('') / \"raw-img\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlw4jyja59AQ"
      },
      "source": [
        "## EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9AGB73N2zGd"
      },
      "outputs": [],
      "source": [
        "df = make_data_frame(data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhhHrtjq6t89",
        "outputId": "6a109811-3fed-40dc-96e7-2db72a2ad55a"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "KiDR1buB76AA",
        "outputId": "62480470-2c93-48a8-976a-30adb9fdd0d3"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fT8C9TFM61kr"
      },
      "source": [
        "Our dataset contains over 26000 pictures of 10 different animals. The most number of sample images are for the class dog. We will visualise the distribution of the whole dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "rvF3aP0K8U78",
        "outputId": "6526ea56-a426-4c61-dcfb-7ddfbd580ab5"
      },
      "outputs": [],
      "source": [
        "print(\"Total images:\", len(df))\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "sns.countplot(x='label', data=df)\n",
        "plt.title(\"Class distribution\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-Fu4URdOToo"
      },
      "source": [
        "We can see that spider and dog images are the most abundant, while the counts for other animals are mostly the same."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_u6evGuAGFx"
      },
      "source": [
        "Next we will check if we have any corrupted files # TODO: too long"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "moFv3EG3A0cd"
      },
      "outputs": [],
      "source": [
        "# from PIL import UnidentifiedImageError\n",
        "\n",
        "# # Check if all images are valid\n",
        "# bad_images = []\n",
        "# for path in df['path']:\n",
        "#     try:\n",
        "#         Image.open(path).verify()\n",
        "#     except UnidentifiedImageError:\n",
        "#         bad_images.append(path)\n",
        "\n",
        "# print(\"Number of bad images:\", len(bad_images))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KsY1AZu82Fg"
      },
      "source": [
        "To verify image quality we will inspect random images from each class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "FPzO5XKG8_P-",
        "outputId": "e3544a38-68ac-4ec1-c5aa-f721e650c652"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 10, figsize=(15,3))\n",
        "classes = df['label'].unique()\n",
        "\n",
        "for i, c in enumerate(random.sample(list(classes), k=10)):\n",
        "    sample_path = random.choice(df[df['label']==c]['path'].values)\n",
        "    img = Image.open(sample_path)\n",
        "    axes[i].imshow(img)\n",
        "    axes[i].set_title(c)\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nval0oMf1i5y"
      },
      "source": [
        "## Experiment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UU_jUaY81V6h"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"batch_size\": 64,\n",
        "    \"learning_rate\": 0.0003,\n",
        "    \"epochs\": 15,\n",
        "    \"img_size\": 224\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZdgYP103xdz"
      },
      "outputs": [],
      "source": [
        "# TODO: used only with pytorch\n",
        "def get_metrics(total_loss, batch_count, preds, ys):\n",
        "    # Average loss in the given epoch\n",
        "    avg_loss = total_loss / batch_count\n",
        "\n",
        "    accuracy = accuracy_score(ys, preds)\n",
        "\n",
        "    # For multi-class classification, use 'weighted' average for f1_score\n",
        "    f1 = f1_score(ys, preds, average='weighted')\n",
        "    precision = precision_score(ys, preds, average='weighted')\n",
        "    recall = recall_score(ys, preds, average='weighted')\n",
        "\n",
        "    return {\n",
        "        \"loss\": avg_loss,\n",
        "        \"accuracy\": accuracy,\n",
        "        \"f1_score\": f1,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJ-Sji3hTa_2"
      },
      "source": [
        "## Data split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03u4Ejgpe2fl"
      },
      "source": [
        "We need to split the data before starting with the preprocessing and augmentation. We opted for a 70-30 split, creating a new split folder storing both training and validation data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-mLJNyETdQ0"
      },
      "outputs": [],
      "source": [
        "# Set new folder locations\n",
        "split_dir = Path('./split')\n",
        "train_dir = split_dir / 'train'\n",
        "val_dir = split_dir / 'val'\n",
        "\n",
        "# Create new folders\n",
        "train_dir.mkdir(parents=True)\n",
        "val_dir.mkdir(parents=True)\n",
        "\n",
        "# Set split ration\n",
        "ratio = 0.7\n",
        "random.seed(23)\n",
        "\n",
        "for animal_class in data_dir.iterdir():\n",
        "  if animal_class.is_dir():\n",
        "    images = [p for p in animal_class.glob(\"*\") if p.is_file()]\n",
        "    random.shuffle(images)\n",
        "\n",
        "    n_train = int(len(images) * ratio)\n",
        "\n",
        "    train_images = images[:n_train]\n",
        "    val_images = images[n_train:]\n",
        "\n",
        "    # Destination folders for current class\n",
        "    train_class_dir = train_dir / animal_class.name\n",
        "    val_class_dir = val_dir / animal_class.name\n",
        "    train_class_dir.mkdir(parents=True)\n",
        "    val_class_dir.mkdir(parents=True)\n",
        "\n",
        "    # Copy images to destination\n",
        "    for img in train_images:\n",
        "      shutil.copy(img, train_class_dir / img.name)\n",
        "\n",
        "    for img in val_images:\n",
        "      shutil.copy(img, val_class_dir / img.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bw9HtQkFgYo_"
      },
      "source": [
        "We will make sure the split worked as intended."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "l3mQ9F05SSGq",
        "outputId": "26f01238-9e40-4150-c59c-e4c28634af8c"
      },
      "outputs": [],
      "source": [
        "train_df = make_data_frame(train_dir)\n",
        "print(\"Total images:\", len(train_df))\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "sns.countplot(x='label', data=train_df)\n",
        "plt.title(\"Training class distribution\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "oJCkES2yS5L6",
        "outputId": "95d5a3da-546d-4071-8bec-63437d1f6066"
      },
      "outputs": [],
      "source": [
        "val_df = make_data_frame(val_dir)\n",
        "print(\"Total images:\", len(val_df))\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "sns.countplot(x='label', data=val_df)\n",
        "plt.title(\"Validation class distribution\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWPNtznlPIGE"
      },
      "source": [
        "\n",
        "## Image preprocessing and augmentations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2xVX9WXPLht"
      },
      "outputs": [],
      "source": [
        "# # Training dataset\n",
        "# train_transform = v2.Compose([\n",
        "#     v2.Resize((224, 224)),\n",
        "#     v2.RandomHorizontalFlip(0.5),\n",
        "#     v2.RandomRotation(15),\n",
        "#     v2.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
        "#     v2.ToImage(),\n",
        "#     v2.ToDtype(torch.float32, scale=True),\n",
        "#     # TODO:\n",
        "#     # v2.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "#     #             std=[0.229, 0.224, 0.225])\n",
        "# ])\n",
        "\n",
        "# train_dataset = datasets.ImageFolder(\n",
        "#     root=train_dir,\n",
        "#     transform=train_transform\n",
        "# )\n",
        "\n",
        "# train_loader = DataLoader(\n",
        "#     train_dataset,\n",
        "#     batch_size=config[\"batch_size\"],\n",
        "#     shuffle=True,\n",
        "# )\n",
        "\n",
        "# # Validation dataset\n",
        "# val_transform = v2.Compose([\n",
        "#     v2.Resize((224, 224)),\n",
        "#     v2.ToImage(),\n",
        "#     v2.ToDtype(torch.float32, scale=True),\n",
        "#     # v2.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "#     #             std=[0.229, 0.224, 0.225])\n",
        "# ])\n",
        "\n",
        "# val_dataset = datasets.ImageFolder(\n",
        "#     root=val_dir,\n",
        "#     transform=val_transform\n",
        "# )\n",
        "\n",
        "# val_loader = DataLoader(\n",
        "#     val_dataset,\n",
        "#     batch_size=config[\"batch_size\"],\n",
        "#     shuffle=False,\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9av7Hdt3tIrQ"
      },
      "source": [
        "## Model\n",
        "We define our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVksWh975ozh"
      },
      "outputs": [],
      "source": [
        "# class AnimalModel(nn.Module):\n",
        "#     def __init__(self, num_classes=10):\n",
        "#         super().__init__()\n",
        "#         self.stack = nn.Sequential(\n",
        "#             nn.Conv2d(3, 64, kernel_size=3, padding=0),\n",
        "#             nn.ReLU(),\n",
        "#             nn.MaxPool2d(2),\n",
        "\n",
        "#             nn.Conv2d(64, 32, kernel_size=3, padding=0),\n",
        "#             nn.ReLU(),\n",
        "#             nn.MaxPool2d(2),\n",
        "\n",
        "#             nn.Conv2d(32, 16, kernel_size=3, padding=0),\n",
        "#             nn.ReLU(),\n",
        "#             nn.MaxPool2d(2),\n",
        "\n",
        "#             nn.Flatten(),\n",
        "#             # TODO: calculate size after changing conv layers\n",
        "#             nn.Linear(16 * 26 * 26, 128),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Linear(128, num_classes)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         return self.stack(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-o_2T7yy5p0f"
      },
      "source": [
        "## Training\n",
        "We begin trainig."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrBfgfsK2Es-"
      },
      "outputs": [],
      "source": [
        "# # Init wandb\n",
        "# with wandb.init(project=\"zneus-2\", config=config) as run:\n",
        "#   model = AnimalModel()\n",
        "#   device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#   print(f\"Running on {device}\")\n",
        "#   model.to(device)\n",
        "\n",
        "#   # TODO: check other loss functions?\n",
        "#   loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "#   # TODO: other optimizers?\n",
        "#   optimizer = torch.optim.Adam(params=model.parameters(), lr=config[\"learning_rate\"])\n",
        "\n",
        "#   # Log both parameters and gradient\n",
        "#   run.watch(model, criterion=loss_fn, log_freq=100, log=\"all\")\n",
        "\n",
        "#   # Start training\n",
        "#   for epoch_idx in range(config[\"epochs\"]):\n",
        "#       epoch_start_time = time.time()\n",
        "#       print(f\"epoch {epoch_idx + 1}\")\n",
        "\n",
        "#       # Train\n",
        "#       train_total_loss = 0\n",
        "#       train_preds = []\n",
        "#       train_ys = []\n",
        "\n",
        "#       model.train()\n",
        "#       # Include dataloader time\n",
        "#       batch_start_time = time.time()\n",
        "#       for batch_idx, (X, y) in enumerate(train_loader):\n",
        "#           #print(f\"batch {batch_idx + 1}\")\n",
        "#           # Move to device\n",
        "#           X = X.to(device)\n",
        "#           y = y.to(device)\n",
        "\n",
        "#           # Predict\n",
        "#           pred = model(X)\n",
        "#           loss = loss_fn(pred, y)\n",
        "\n",
        "#           # Optimize\n",
        "#           loss.backward()\n",
        "#           optimizer.step()\n",
        "#           optimizer.zero_grad()\n",
        "\n",
        "#           # Save values\n",
        "#           train_total_loss += loss.item()\n",
        "#           train_preds.extend(torch.argmax(torch.softmax(pred, dim=1), dim=1).detach().cpu().numpy().ravel())\n",
        "#           train_ys.extend(y.detach().cpu().numpy().ravel())\n",
        "\n",
        "#           #print(f\"batch took: {time.time() - batch_start_time:.4f}s\")\n",
        "#           #batch_start_time = time.time()\n",
        "\n",
        "#       train_metrics = get_metrics(\n",
        "#           train_total_loss, len(train_loader), train_preds, train_ys\n",
        "#       )\n",
        "#       print(\"training:\")\n",
        "#       for k, v in train_metrics.items():\n",
        "#           print(f\"{k}: {v}\")\n",
        "#       print()\n",
        "\n",
        "#       # Validate\n",
        "#       val_total_loss = 0\n",
        "#       val_preds = []\n",
        "#       val_ys = []\n",
        "\n",
        "#       model.eval()\n",
        "#       with torch.no_grad():\n",
        "#           for X, y in val_loader:\n",
        "#               # Move to device\n",
        "#               X = X.to(device)\n",
        "#               y = y.to(device)\n",
        "\n",
        "#               # Predict\n",
        "#               pred = model(X)\n",
        "#               loss = loss_fn(pred, y)\n",
        "\n",
        "#               # Save values\n",
        "#               val_total_loss += loss.item()\n",
        "#               val_preds.extend(torch.argmax(torch.softmax(pred, dim=1), dim=1).detach().cpu().numpy().ravel())\n",
        "#               val_ys.extend(y.detach().cpu().numpy().ravel())\n",
        "\n",
        "\n",
        "#       val_metrics = get_metrics(\n",
        "#           val_total_loss, len(val_loader), val_preds, val_ys\n",
        "#       )\n",
        "#       print(\"validation:\")\n",
        "#       for k, v in val_metrics.items():\n",
        "#           print(f\"{k}: {v}\")\n",
        "#       print()\n",
        "\n",
        "#       print(f\"epoch took: {time.time() - epoch_start_time:.2f}s\")\n",
        "\n",
        "#       # Wandb logging\n",
        "#       run.log(\n",
        "#           {\n",
        "#               **{f\"train_{k}\": v for k, v in train_metrics.items()},\n",
        "#               **{f\"val_{k}\": v for k, v in val_metrics.items()},\n",
        "#           }\n",
        "#       )\n",
        "\n",
        "#   # TODO: add test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Wdlt7Dw3gKm"
      },
      "outputs": [],
      "source": [
        "# Data augmentations\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "    tf.keras.layers.RandomRotation(0.1),\n",
        "    tf.keras.layers.RandomContrast(0.1),\n",
        "    tf.keras.layers.Lambda(\n",
        "        lambda x: tf.image.random_brightness(x, max_delta=0.1)\n",
        "    ),\n",
        "    tf.keras.layers.Lambda(\n",
        "        lambda x: tf.image.random_saturation(x, lower=0.9, upper=1.1)\n",
        "    ),\n",
        "    tf.keras.layers.Lambda(\n",
        "        lambda x: tf.image.random_hue(x, max_delta=0.1)\n",
        "    ),\n",
        "], name=\"data_augmentation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1N-RePM3-Lx"
      },
      "outputs": [],
      "source": [
        "# Calculate class weights\n",
        "labels = df['label']\n",
        "classes = sorted(labels.unique())\n",
        "class_counts = labels.value_counts().sort_index()\n",
        "\n",
        "total = class_counts.sum()\n",
        "num_classes = len(classes)\n",
        "\n",
        "class_weight = {\n",
        "    i: total / (num_classes * class_counts)\n",
        "    for i, count in enumerate(class_counts)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsuERePfiILb",
        "outputId": "97e68e88-c49c-4e90-c658-b4b4f652617f"
      },
      "outputs": [],
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "if tf.config.list_physical_devices('GPU'):\n",
        "  print(\"GPU is available and being used.\")\n",
        "else:\n",
        "  print(\"No GPU available. Please change the runtime type to GPU.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAM7knUPiKxG",
        "outputId": "622181ed-1641-433a-d753-844cd7a835db"
      },
      "outputs": [],
      "source": [
        "train_data = tf.keras.utils.image_dataset_from_directory(\n",
        "  train_dir,\n",
        "  # TODO: remove as data is split before-hand\n",
        "  # validation_split=0.2,\n",
        "  # subset=\"training\",\n",
        "  seed=123,\n",
        "  image_size=(config[\"img_size\"], config[\"img_size\"]),\n",
        "  batch_size=config[\"batch_size\"])\n",
        "\n",
        "val_data = tf.keras.utils.image_dataset_from_directory(\n",
        "  val_dir,\n",
        "  # validation_split=0.2,\n",
        "  # subset=\"validation\",\n",
        "  seed=123,\n",
        "  image_size=(config[\"img_size\"], config[\"img_size\"]),\n",
        "  batch_size=config[\"batch_size\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oN2sLPjeiO5l"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "# TODO: cache on disk, cache in ram crashes session\n",
        "# train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "# val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "train_ds = train_data.prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_data.prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xQDu67kiQVI"
      },
      "outputs": [],
      "source": [
        "# TODO: add wandb logging -> https://docs.wandb.ai/models/tutorials/keras_models\n",
        "num_classes = 10\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  data_augmentation,\n",
        "  tf.keras.layers.Rescaling(1./255),\n",
        "  tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(),\n",
        "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(),\n",
        "  tf.keras.layers.Conv2D(16, 3, activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(num_classes)\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "  optimizer='adam',\n",
        "  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "  metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "4DU8BlRN5Pzg",
        "outputId": "922ba07b-c5b1-4e0f-c61e-34894f40ea3b"
      },
      "outputs": [],
      "source": [
        "# Intiate wandb\n",
        "wandb.init(project=\"zneus-2\", config=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "S1osY5iWiR2v",
        "outputId": "7e9efc7a-04df-4b03-ad58-4b6fc75dbd44"
      },
      "outputs": [],
      "source": [
        "model.fit(\n",
        "  train_data,\n",
        "  validation_data=val_data,\n",
        "  epochs=config[\"epochs\"],\n",
        "  # class_weight=class_weight,\n",
        "  # callbacks=[WandbCallback(\n",
        "  #     save_model=True,\n",
        "  #     log_weights=True,\n",
        "  #     log_evaluation=True\n",
        "  # )]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmG2cBrR5z2h"
      },
      "outputs": [],
      "source": [
        "# Create confusion matrix\n",
        "y_true = np.concatenate([y.numpy() for _, y in val_data])\n",
        "y_pred = np.argmax(model.predict(val_data), axis=1)\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "labels = val_data.class_names\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8bpHShnVUKE"
      },
      "source": [
        "early stopping, meaningful changes based on convergence, overfitting, loss"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "qlw4jyja59AQ"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
